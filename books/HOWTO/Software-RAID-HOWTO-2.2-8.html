<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<HTML>
<HEAD>
 <META NAME="GENERATOR" CONTENT="SGML-Tools 1.0.9">
 <TITLE>Software-RAID HOWTO: Производительность, утилиты и общие ключевые вопросы</TITLE>
 <LINK HREF="Software-RAID-HOWTO-2.2-9.html" REL=next>
 <LINK HREF="Software-RAID-HOWTO-2.2-7.html" REL=previous>
 <LINK HREF="Software-RAID-HOWTO-2.2.html#toc8" REL=contents>
</HEAD>
<BODY>
<A HREF="Software-RAID-HOWTO-2.2-9.html">Next</A>
<A HREF="Software-RAID-HOWTO-2.2-7.html">Previous</A>
<A HREF="Software-RAID-HOWTO-2.2.html#toc8">Contents</A>
<HR>
<H2><A NAME="s8">8. Производительность, утилиты и общие ключевые вопросы</A></H2>

<P>
<OL>
<LI><B>В</B>:
Я создал RAID-0 устройство на <CODE>/dev/sda2</CODE> и
<CODE>/dev/sda3</CODE>. Устройство намного медленнее, чем отдельный раздел. 
md - это куча мусора?
<BLOCKQUOTE>
<B>О</B>:
Для запуска устройства RAID-0 на полную скорость, у Вас должны
разделы на разных дисках.  Кроме того, помещая две половины зеркала
на один диск Вы не получаете защиты от отказа диска.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Зачем использовать линейный RAID, если RAID-0 делает то же самое,
но с лучшей производительностью?
<BLOCKQUOTE>
<B>О</B>:
Не очевидно, что RAID-0 даст большую производительность; 
фактически, в некоторых случаях, он может сделать хуже.
Файловая система ext2fs распределяет фалы по всему разделу,
и пытается хранить все блоки файла вместе, в основном в целях 
избежания фрагментации.  
Таким образом, ext2fs ведет себя "как если бы" stripe-ы были 
(переменного размера) размером с файл. Если есть несколько 
дисков соединяются в один линейный RAID, это приведет к 
статистическому распределению файлов на оба диска.  Таким образом,
по крайней мере для ext2fs, линейный RAID работает во многом 
подобно RAID-0 с большим размером stripe.  Наоборот, RAID-0 
с маленьким размером stripe при одновременном доступе к нескольким 
большим файлам может вызвать излишнюю дисковую активность, 
приводящую к снижению производительности.  
<P>Во многих случаях, RAID-0 может явно выигрывать. Например,
представьте большой файл базы данных.  Так как ext2fs пытается 
объединить вместе все блоки файла, велики шансы, что она заполнит 
только одно устройство при использовании линейного RAID, но
будет разделять на много кусочков, при использовании RAID-0.
Теперь представим несколько нитей (ядра) пытающихся получить
произвольный доступ к базе данных.  При линейном RAID, весь
доступ пойдет на один диск, что не так эффективно, как
параллельный доступ, создаваемый RAID-0.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Как RAID-0 обрабатывает ситуацию, где stripe-ы на различных
разделах разного размера?  stripe-ы распределяются однообразно?

<BLOCKQUOTE>
<B>О</B>:
Для понимания этого, давайте рассмотрим пример с тремя разделами; 
50Мб, 90Мб и 125Мб.

Назовем D0 50Мб диск, D1 90Мб диск и D2 125Мб диск.  Когда Вы 
запускаете устройство, драйвер вычисляет 'strip zones'.  В этом 
случае, он найдет 3 зоны, определенные подобно этому:

<PRE>
            Z0 : (D0/D1/D2) 3 x 50 = 150MB  всего в этой зоне
            Z1 : (D1/D2)  2 x 40 = 80MB всего в этой зоне
            Z2 : (D2) 125-50-40 = 35MB всего в этой зоне.
            
</PRE>


Вы можете видеть, что общий размер зон - размер виртуального 
устройства, но, в зависимости от зоны, striping различается.
Z2 особенно неэффективна,так как там только один диск.

Так как <CODE>ext2fs</CODE> и большинство других файловых систем Unix  
распределяют файлы по всему диску, У Вас 35/265 = 13% шансов, что 
заполнение закончится на Z2, и не получится никаких преимуществ 
RAID-0.
           
(DOS пытается заполнить диск от начала до конца, и таким образом,
старые файлы должны храниться на Z0.  Однако, эта стратегия
приводит к резкой фрагментации файловой системы,
это причина того, что никто кроме DOS так не делает.)
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
У меня есть жесткий диск производителя X и контроллер производителя Y
и я предполагаю использовать <CODE>md</CODE>.
Это даст значительное увеличение производительности?
Производительность в самом деле заметна?

<BLOCKQUOTE>
<B>О</B>:
Ответ зависит от используемой Вами конфигурации.
<P>
<DL>
<DT><B>Производительность Linux MD RAID-0 и линейного RAID:</B><DD><P>Если система слишком загружена вводом-выводом,
статистически, часть пойдет на один диск, а часть на другой.
Таким образом, производительность увеличится по сравнению с
одиночным диском.  Фактическое увеличение сильно зависит
от текущих данных, размера stripe, и других факторов.
В системе с низким вводом-выводом, производительность
эквивалентна производительности одного диска.
<P>
<P>
<DT><B>Производительность чтения Linux MD RAID-1 (зеркализация):</B><DD><P>MD реализует балансировку чтения. То есть, код RAID-1
будет поочередно выбирать каждый из дисков (двух или более)
зеркала, производя поочередное чтение с каждого диска.
В случае небольшого ввода-вывода, это вовсе не изменит 
производительность: Вы будете ждать завершения чтения одного
диска.
Но, с двумя дисками и при высокой загрузке вводом-выводом,
возможно получить практически удвоенную производительность,
так как опреации чтения будут выполняться с каждого диска 
одновременно.
Для N дисков в зеркале, это может увеличить 
производительность в N раз.
<P>
<DT><B>Производительность записи Linux MD RAID-1 (зеркализация):</B><DD><P>Нужно ждать, пока запишутся данные на все диски зеркала. 
Это из-за того, что копия данных должна быть записана
на каждый из дисков зеркала.
Таким образом, производительность будет приблизительно 
эквивалентна производительности записи на один диск.
<P>
<DT><B>Производительность чтения Linux MD RAID-4/5:</B><DD><P>Статистически, данный блок может быть на любом из дисков, 
и, таким образом, производительность чтения RAID-4/5
во многом подобна RAID-0.  Она зависит от данных, 
размера stripe, и приложения.  Она не будет так хороша, как
производительность чтения в зеркальном массиве.
<P>
<DT><B>Производительность записи Linux MD RAID-4/5:</B><DD><P>Она, в общем, должна быть предположительно меньше, чем у 
одного диска.  Это из-за того, что на один диск 
должна быть записана информация о паритете, в то время 
как на другой - данные.  Однако,
в случае вычисления нового паритета, старый паритет и  
старые данные должны быть сначала считаны. Старые данные, 
новые данные и старый паритет должны быть объединены 
операцией XOR для определения новой информации о паритете:
это требует циклов процессора и дополнительного 
доступа к дискам.
</DL>
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Какую конфигурацию RAID я должен использовать для оптимальной производительности?
<BLOCKQUOTE>
<B>О</B>:
Ваша цель максимальная пропускная способность, или минимальные 
время доступа?
Нет простого ответа, так как на производительность влияет много факторов:

<UL>
<LI>операционная система - будет один процесс/нить, 
выполнять доступ к диску или несколько?</LI>
<LI>приложение - выполняется доступ к данным последовательно 
или с произвольно?</LI>
<LI>файловая система - группируются файлы или 
рассредотачиваются ( ext2fs группирует блоки файлов,
и рассредотачивает сами файлы)</LI>
<LI>драйвер диска - количество блоков упреждающего чтения 
(это настраиваемый параметр)</LI>
<LI>CEC аппаратура - один дисковый контроллер или несколько?</LI>
<LI>контроллер жесткого диска - может ли выполнять множество 
запросов или нет? Имеет ли кеш?</LI>
<LI>Жесткий дик  - размер памяти кеш-буфера -- Достаточен ли 
будет для обработки размеров записей и желаемой частоты 
обращений?</LI>
<LI>физическая организация - количество блоков в цилиндре -- 
доступ к блокам на различных цилиндрах приведет к 
пере-позиционированию головки.</LI>
</UL>
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Какая оптимальная конфигурация RAID-5 для производительности?
<BLOCKQUOTE>
<B>О</B>:
Так как RAID-5 создает загрузку ввода-вывода, которая
одинаково распределена на несколько устройств, лучшая 
производительность будет получена, когда 
RAID набор сбалансирован использованием  
идентичных дисков, идентичных контроллеров,  и
одинаковым (небольшим) числом дисков на каждом контроллере.

Однако заметьте, что использование идентичных компонент 
увеличивает возможность множества одновременных отказов,
например из-за внезапного толчка или урона, перегрева,
или скачка электричества во время грозы. Смешивание марок и моделей
помогает минимизировать этот риск.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Какой оптимальный размер блока для массива RAID-4/5?

<BLOCKQUOTE>
<B>О</B>:
Если используется текущая (Ноябрь 1997) RAID-4/5
реализация, строго рекомендуется создавать файловую систему с
<CODE>mke2fs -b 4096</CODE> вместо 1024 байтов, по умолчанию.

<P>Это потому, что текущая реализация RAID-5
резервирует одну 4Кб страницу памяти на дисковый блок;
если размер блока диска будет 1Кб, тогда
75% памяти, которую резервирует RAID-5 для осуществления 
ввода-вывода, не используется.
Если размер блока диска совпадает с размером страницы памяти, тогда
драйвер (потенциально) может использовать всю страницу.
Итак, для файловой системы с размером блока 4096
в отличие от системы с размером блока 1024, драйвер RAID будет
потенциально ставить в очередь 4 раза 
производя ввод-вывод с драйверам нижнего уровня без 
расходования дополнительной памяти.
<P>
<P><B>Заметка:</B> пометки выше НЕ применимы драйверу программного
RAID-0/1/линейного.
<P>
<P><B>Заметка:</B> высказывание о 4Кб странице памяти применимо к 
архитектуре Intel x86.  Размер страницы на Alpha, Sparc, и других 
процессорах различается; я думаю на Alpha/Sparc он 8Кб (????).
Скорректируйте соответственно указанное значение.
<P>
<P><B>Заметьте:</B> если на Вашей файловой системе много небольших
файлов (файлов размером менее 10Кб), значительная чaсть 
дискового пространства может быть потеряна.  Это из-за того,
что файловая система распределяет дисковое пространство  
частями размером в блок.  Выделение больших блоков маленьким файлам 
приводит к потерям дискового пространства: таким образом, Вы можете
поставить небольшой размер блока, получить большую эффективность
использования емкости, и не беспокоиться о "потерянной" памяти
из-за несоответствия размера блока размеру страницы памяти.
<P>
<P><B>Заметка:</B> большинство ''типичных'' систем не содержат много
маленьких файлов.  То есть, хотя могут быть тысячи небольших файлов,
это будет приводить к потере только от 10 до 100Мб, что, возможно,
приемлимо, учитывая производительность, на много-гигабайтном диске.
<P>Однако, для серверов новостей, может быть десятки и сотни
тысяч небольших файлов.  В этом случае, меньший размер блока,
и таким образом сохраненная емкость, может быть более важной,
чем более эффективный ввод-вывод.
<P>
<P><B>Заметка:</B> существует экспериментальная файловая система 
для Linux, которая пакует маленькие фалы и группы файлов в один 
блок. Она имеет большую производительность, если средний
размер файла намного меньше размера блока.
<P>
<P><B>Заметка:</B> Будущие версии могут реализовать схемы, которые
лишат смысла вышеприведенную дискуссию. Однако, это сложно 
реализовать, так как динамическое распределение на ходу может 
привести к мертвым-блокировкам (dead-locks); текущая реализация
выполняет статическое предварительное выделение.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Как размер куска (размер stripe) влияют на производительность  
моего RAID-0, RAID-4 или RAID-5 устройства?

<BLOCKQUOTE>
<B>О</B>:
Размер куска - количество смежных данных на виртуальном  
устройстве, которы смежные и на физическом устройстве. 
В этом HOWTO, "кусок" и "stripe" подразумевают одно и то же: 
что часто называется "stripe" в другой документации по 
RAID, в MD man страницах называется "кусок" ("chunk"). 
Stripe-ы или куски применимы только к 
RAID 0, 4 и 5, так как stripe-ы не используются в 
зеркализации (RAID-1) и простом соединении (линейный RAID).
Размеры stripe влияют на задержку, пропускную способность,
и конкуренцию между отдельными операциями
(возможность одновременного обслуживания перекрывающихся запросов
ввода-вывода).
<P>Предполагая использование файловой системы ext2fs, и текущих 
правил ядра для упреждающего чтения, большие размеры stripe почти
всегда лучше, чем маленькие размеры, и размеры stripe 
от почти четырех до полного цилиндра диска наилучшие. 
Чтобы понять это требование, рассмотрим  
эффективность больших stripe на маленьких файлах, и маленьких 
stripe на больших файлах.  Размер stripe не влияет на 
производительность чтения на маленьких файлах:  для
массива из N дисков, файл имеет 1/N вероятность  
попасть целиком в один stripe на любой диск. 
Таким образом, и задержка и производительность чтения сравнима
с чтением одного диска.  Предположим, что маленькие файлы
статистически хорошо распределяются по файловой системе,
(и, на файловой системе ext2fs, они дожны), грубо в N раз более
упорядочены, конкурентные чтения должны быть возможны
без значительных коллизий между ними.  Наоборот, если  
используются очень маленького размера stripe-ы, и последовательно
читается большой файл, то чтение будет выдаваться всем дискам 
массива. Для чтения одного большого файла, задержка будет 
почти двойная, так как увеличивается вероятность нахождения блока в
трех четвертях оборота диска или далее. 
Однако заметьте аргумент: пропускная способность может увеличиться 
почти 
в N раз для чтения одного большого файла, так как N дисков могут
читать одновременно (то есть, если используется упреждающее чтение,
то все диски остаются активными).  Но есть другй
контр-аргумент:  если все диски уже заняты
чтением файла, то попытки одновременного чтения второго или третьего
файла приведут к значительной борьбе,
разрушив производительность, так как алгоритмы управления диском 
будут двигать головками вдоль пластины. Таким образом, большие 
stripe-ы будут почти всегда приводить к большей производительности. 
Единственное исключение  - случай, при использовании хорошего 
алгоритма упреждающего чтения, где один поток в одно время 
читает один большой файл, и он требует наивысшей возможной 
производительности. В этом случае желательны небольшие stripe-ы.
<P>
<P>Заметьте, что этот HOWTO ранее рекомендовал небольшие размеры 
stripe-ов для спула новостей или других систем с множеством мелких
файлов. Это плохой совет, и вот почему:  спулы новостей 
содержат не только много маленьких файлов, но также и большие
суммарные файлы, также как и большие каталоги.  Если суммарный файл
более одного stripe, его чтение задействует много дисков,
замедляя все, так как каждый диск выполняют позиционирование.  
Подобным образом, текущая файловая система ext2fs
просматривает каталоги в линейной, последовательной
манере.  Таким образом, чтобы найти данный файл или inode, 
в средней части будет прочитана половина каталога. 
Если этот каталог простирается на несколько  
stripe-ов (несколько дисков), чтение 
каталога (такое как при команде ls) будет очень медленным. 
Спасибо Steven A. Reisman 
&lt;
<A HREF="mailto:sar@pressenter.com">sar@pressenter.com</A>&gt; за эту поправку.
Steve также добавил следующее:
<BLOCKQUOTE>
Я обнаружил, что использование 256k stripe дает намного лучшую 
производительность. 
Я подозреваю, что оптимальный размер должен быть размером с  
цилиндр диска (или, может быть размером с кеш диска).  
Однако, современные диски содержат зоны с различным  
количеством секторов (и размер кеша варьируется в зависимости от
модели диска).  Невозможно гарантировать, что stripe-ы не будут 
пересекать границу цилиндра.
</BLOCKQUOTE>
<P>
<P>
<P>Утилиты позволяют задавать размер в Кбайтах.
Вы можете указать его величиной с размер страницы Вашего 
CPU (4Кб на x86).  
<P>
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Каков правильный stride при создании файловой системы 
ext2fs на разделе RAID?  Под stride я подразумеваю 
-R флаг в команде <CODE>mke2fs</CODE>:
<PRE>
mke2fs -b 4096 -R stride=nnn  ... 
        
</PRE>

Кокое должно быть значение nnn?
<BLOCKQUOTE>
<B>О</B>:
Флаг <CODE>-R stride</CODE> используется, чтобы указать файловой 
системе размер RAID stripe-ов.  Так как только RAID-0,4 и 5
использует stripe-ы, а RAID-1 (зеркализация) и линейный RAID не
используют, этот флаг применим только к RAID-0,4,5.

Знание размера stripe-а позволяет <CODE>mke2fs</CODE>
выделять блок и битовый поля inode так, что они не все хранятся  
на одном физическом устройстве.  Неизвестный помощник написал:
<BLOCKQUOTE>
Прошлой весной я заметил, что один диск из пары всегда больше
занят вводом-выводом, и отследил - это из-за этих блоков 
мета-данных. Ted добавил опцию <CODE>-R stride=</CODE>, в мой вариант
ответа и предложение обходного варианта.
</BLOCKQUOTE>

Для файловой системы с блоком 4Кб, с размером stripe в 256Кб, 
нужно использовать <CODE>-R stride=64</CODE>.
<P>Если Вы не доверяете флагу <CODE>-R</CODE>, Вы можете получить подобный
эффект другим путем.   Steven A. Reisman 
&lt;
<A HREF="mailto:sar@pressenter.com">sar@pressenter.com</A>&gt; написал:
<BLOCKQUOTE>
Другое соображение - файловая система используемая на устройстве
RAID-0.
Файловая система выделяет ext2 8192 блоков в группу. У каждой
группы есть свой набор inode-ов. Если есть 2, 4 или 8 дисков, 
эти inode скапливаются на первом диске.  Я распределили inode-ы
по всем дискам, указав mke2fs выделять только 7932 блоков на 
группу.  
</BLOCKQUOTE>

Некоторые страницы mke2fs не описывают флаг 
<CODE>[-g blocks-per-group]</CODE>
используемый при этой операции.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Где в загрузочных скриптах я могу вставить команду <CODE>md</CODE>,
так, чтобы все автоматически стартовало в процессе загрузки?

<BLOCKQUOTE>
<B>О</B>:
Rod Wilkens
&lt;
<A HREF="mailto:rwilkens@border.net">rwilkens@border.net</A>&gt;
написал:
<BLOCKQUOTE>
Вот что я сделал: вставил ``<CODE>mdadd -ar</CODE>'' в
``<CODE>/etc/rc.d/rc.sysinit</CODE>'' прямо после загрузки модулей,
и перед  проверкой дисков ``<CODE>fsck</CODE>''.
Таким образом, Вы можете вставить устройство 
``<CODE>/dev/md?</CODE>'' в ``<CODE>/etc/fstab</CODE>''. Затем вставить
``<CODE>mdstop -a</CODE>'' прямо после де-монтирования всех 
дисков ``<CODE>umount -a</CODE>'', в файле
``<CODE>/etc/rc.d/init.d/halt</CODE>''.
</BLOCKQUOTE>

Для raid-5, Вы должны посмотреть на код возврата <CODE>mdadd</CODE>, 
и если он ошибочен, сделать 
<BLOCKQUOTE><CODE>
<PRE>
ckraid --fix /etc/raid5.conf
            
</PRE>
</CODE></BLOCKQUOTE>

для восстановления любых повреждений.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Меня интересует возможно ли установить striping для более, чем  
2 устройств в <CODE>md0</CODE>? Это для сервера новостей,
и у меня есть 9 дисков... Нужно ли говорить, что мне нужно больше, чем
два. Это возможно?

<BLOCKQUOTE>
<B>О</B>:
Да. (описать как это сделать)
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Когда программный RAID превосходит аппаратный RAID?
<BLOCKQUOTE>
<B>О</B>:
Обычно, аппаратный RAID считается производительнее программного 
RAID, так как аппаратные контроллеры, часто содержат большой кеш,
и могут лучше выполнять планирование параллельных операций записи.
Однако, интегрированный программный RAID может (и дает) определенное
преимущество при реализации в операционной системе.

<P>Например, ... мммм. Мы обходим молчанием темное описание 
кеширования реконструированных блоков в буферный кеш ...
<P>
<P>На дуальных PPro SMP системах, мне рассказывали, что 
производительность программного RAID превышала производительность 
плат аппаратного RAID известных производителей с кратностью
от 2 до 5 раз.
<P>
<P>Программный RAID также очень интересная опция для 
избыточных серверных систем высокой готовности. В такой 
конфигурации, два CPU подсоединены к одному набору
SCSI дисков.  Если один север рушится или отказывается отвечать, 
то другой сервер может <CODE>mdadd</CODE>,
<CODE>mdrun</CODE> и <CODE>mount</CODE> массив программного RAID, и 
продолжить работу.  Этот режим работы не всегда возможен с
многими аппаратными RAID контроллерами, из-за состояний конфигурации
которое аппаратные контроллеры могут поддерживать.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Если я обновляю версию моих raidtools, приведет ли это к проблемам  
манипулирования старых raid массивами?  Коротко, должен ли я 
пересоздать мои массивы RAID при обновлении raid утилит?

<BLOCKQUOTE>
<B>О</B>:
Нет, по крайней мере до смены старшего номера версии.
MD версия x.y.z состоит из трех подверсий:
<PRE>
     x:      старший номер версии.
     y:      младший номер версии.
     z:      номер уровня патча.
            
</PRE>


Версия x1.y1.z1 драйвера RAID поддерживает RAID массив с 
версией x2.y2.z2 в случае (x1 == x2) и (y1 >= y2).
        
Различные номера патчей (z) для тех же (x.y) версий 
разработаны по большей мере совместимыми.
        
<P>Младший номер версии увеличивается всякий раз, когда
код RAID массива
изменяется таким образом, что он несовместим с старыми версиями 
драйвера. Новые версии драйвера должны поддерживать совместимость 
с старыми RAID массивами.
<P>Старший номер версии увеличивается, если более не имеет смысла
поддерживать старые RAID массивы в новом коде ядра.
<P>
<P>Для RAID-1, не правдоподобно, чтобы ни дисковый уровень  ни
структура суперблока изменились в ближайшее время.  Скорее всего 
любые оптимизации и новые свойства (реконструкция, многопоточные  
утилиты, горячая замена, и т.п.) не отразятся на физическом 
размещении.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Комманда <CODE>mdstop /dev/md0</CODE> говорит, что устройство занято.

<BLOCKQUOTE>
<B>О</B>:
Есть процесс, который держит открытым файл на <CODE>/dev/md0</CODE>, или
<CODE>/dev/md0</CODE> все еще смонтирован.  Завершите процесс или  
<CODE>umount /dev/md0</CODE>.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Существуют утилиты измерения производительности?
<BLOCKQUOTE>
<B>О</B>:
Существует новая утилита, называемая <CODE>iotrace</CODE> в каталоге 
<CODE>linux/iotrace</CODE>. Она читает <CODE>/proc/io-trace</CODE> и 
анализирует/строит графики из его вывода.  Если Вы чувствуете, что 
блочная производительность Вашей системы слишком низкая, 
просто посмотрите на вывод iotrace.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Я читал исходники RAID, и видел, что там определено значение 
<CODE>SPEED_LIMIT</CODE> равное 1024Кб/сек.  Что это значит?
Это ограничивает производительность?

<BLOCKQUOTE>
<B>О</B>:
<CODE>SPEED_LIMIT</CODE> используется для ограничения скорости 
реконструкции RAID при автоматической реконструкции.  
По существу, автоматическая реконструкция 
позволяет Вам <CODE>e2fsck</CODE> и <CODE>mount</CODE> сразу после 
неправильного завершения,
без предварительного запуска <CODE>ckraid</CODE>.  Автоматическая
реконструкция также используется после замены отказавшего диска.

<P>Для избежания подавления системы при реконструкции, 
нить реконструкции контролирует скорость реконструкции
и уменьшает ее, если она слишком высока.  Предел 1Мб/сек 
был выбран как разумная норма, которая позволяет реконструкции 
завершаться умеренно быстро, при создании только небольшой
нагрузки на систему, не мешая другим процессам.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Как насчет ''синхронизации шпинделей''или ''дисковой синхронизации''?
<BLOCKQUOTE>
<B>О</B>:
Синхронизация шпинделей используется для поддержания вращения 
нескольких дисков с одинаковой скоростью, так что пластины дисков
всегда точно выровнены. Это используется некоторыми аппаратными 
контроллерами для лучшей организации записи на диски.   
Однако, в программном RAID, эта информация не используется,
и синхронизация шпинделей может даже снизить производительность.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Как я могу установить пространства для подкачки используя raid 0?
Должна ли быть striped подкачка  на 4+ дисках быть быстрой?
<BLOCKQUOTE>
<B>О</B>:
Leonard N. Zubkoff отвечает:
Да она действительно быстра, но Вам не нужно использовать MD для 
получения striped подкачки.  Ядро автоматически разделяет 
подкачку по нескольким пространствам подкачки с одинаковым  
приоритетом.  Например, следующие записи из  
<CODE>/etc/fstab</CODE> разделяют подкачку по пяти дискам 
в три группы:

<PRE>
/dev/sdg1       swap    swap    pri=3
/dev/sdk1       swap    swap    pri=3
/dev/sdd1       swap    swap    pri=3
/dev/sdh1       swap    swap    pri=3
/dev/sdl1       swap    swap    pri=3
/dev/sdg2       swap    swap    pri=2
/dev/sdk2       swap    swap    pri=2
/dev/sdd2       swap    swap    pri=2
/dev/sdh2       swap    swap    pri=2
/dev/sdl2       swap    swap    pri=2
/dev/sdg3       swap    swap    pri=1
/dev/sdk3       swap    swap    pri=1
/dev/sdd3       swap    swap    pri=1
/dev/sdh3       swap    swap    pri=1
/dev/sdl3       swap    swap    pri=1
</PRE>
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Я хочу получить максимальную производительность.  Я должен использовать несколько контроллеров?
<BLOCKQUOTE>
<B>О</B>:
Во многих случаях, ответ - да.  Используя несколько 
контроллеров для параллельного доступа к дискам увеличивает  
производительность. Однако, действительное приращение
фактически зависит от вашей конфигурации.  Например,
как сообщили (Vaughan Pratt, Январь 98) что
один 4.3Гб Cheetah подключенный к Adaptec 2940UW 
может дать до 14Мб/сек (без использования RAID).  
Установив два диска на один контроллер, и используя  
конфигурацию RAID-0 привело к увеличению производительности 
до 27 Мб/сек.  

<P>Заметьте,что 2940UW контроллер  - "Ultra-Wide"
SCSI контроллер, теоретически способный к пакетным передачам 
40Мб/сек, так что указанные измерения не неожиданность. 
Однако, более медленный контроллер подключенный к двум
быстрым дискам будет бутылочным горлышком. Также заметьте,
что большинство внешних SCSI подключений (таких как секции
с лотками горячей замены) не могут работать на 40Мб/сек,
из-за проблем с кабелями и электрических шумов.
<P>
<P>Если Вы разрабатываете систему с несколькими контроллерами,
помните, что большинство дисков и контроллеров в среднем работает
на 70-85% их максимальной скорости.
<P>
<P>Также заметьте, что использование одного контроллера на диск
может, по всей вероятности, уменьшить простой системы 
из-за отказа контроллера или кабеля (теоретически --
только в случае правильной обработки драйвером отказа контроллера.
Не все драйвера SCSI представляются способными обрабатывать 
эту ситуацию без паники или иных блокировок). 
</BLOCKQUOTE>
</LI>
</OL>
<HR>
<A HREF="Software-RAID-HOWTO-2.2-9.html">Next</A>
<A HREF="Software-RAID-HOWTO-2.2-7.html">Previous</A>
<A HREF="Software-RAID-HOWTO-2.2.html#toc8">Contents</A>
</BODY>
</HTML>
