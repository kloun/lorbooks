<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<HTML>
<HEAD>
 <META NAME="GENERATOR" CONTENT="SGML-Tools 1.0.9">
 <TITLE>Software-RAID HOWTO: Установка и установочные соображения</TITLE>
 <LINK HREF="Software-RAID-HOWTO-2.2-4.html" REL=next>
 <LINK HREF="Software-RAID-HOWTO-2.2-2.html" REL=previous>
 <LINK HREF="Software-RAID-HOWTO-2.2.html#toc3" REL=contents>
</HEAD>
<BODY>
<A HREF="Software-RAID-HOWTO-2.2-4.html">Next</A>
<A HREF="Software-RAID-HOWTO-2.2-2.html">Previous</A>
<A HREF="Software-RAID-HOWTO-2.2.html#toc3">Contents</A>
<HR>
<H2><A NAME="s3">3. Установка и установочные соображения</A></H2>

<P>
<OL>
<LI><B>В</B>:
Как лучше сконфигурировать программный RAID?
<BLOCKQUOTE>
<B>О</B>:
Я обнаружил, что планирование файловой системы одна из
труднейших задач конфигурирования Unix.
Для ответа на Ваш вопрос, я могу написать, что мы сделаем.

Мы планируем следующую установку:
<UL>
<LI>два EIDE диска, 2.1Гб каждый.
<BLOCKQUOTE><CODE>
<PRE>
диск раздел т.монтирования размер   устройство
  1      1       /        300M   /dev/hda1
  1      2       swap      64M   /dev/hda2
  1      3       /home    800M   /dev/hda3
  1      4       /var     900M   /dev/hda4

  2      1       /root    300M   /dev/hdc1
  2      2       swap      64M   /dev/hdc2
  2      3       /home    800M   /dev/hdc3
  2      4       /var     900M   /dev/hdc4
                    
</PRE>
</CODE></BLOCKQUOTE>
</LI>
<LI>Каждый диск на отдельном контроллере (и отдельном кабеле).
Теоретически отказ контроллера и/или
отказ кабеля не запретит доступ к обоим дискам.
Также, мы возможно сможем получить повышение производительности
от параллельных операций на двух контроллерах/кабелях.
</LI>
<LI>Установим ядро Linux в корневой (<CODE>/</CODE>)
раздел <CODE>/dev/hda1</CODE>.  Пометим этот раздел как загрузочный.
                    </LI>
<LI><CODE>/dev/hdc1</CODE> должен содержать ``холодную'' копию
<CODE>/dev/hda1</CODE>. Это НЕ raid копия,
просто один-в-один копия. Только для использования в качестве 
восстановительного диска в случая отказа основного диска;
пометим <CODE>/dev/hdc1</CODE> как загрузочный, и используем его 
для хранения без переустановки системы.
Вы можете также поместить копию <CODE>/dev/hdc1</CODE>
ядра в LILO для упрощения загрузки в случае отказа.

Теоретически, в случае отказа, так я все еще могу загрузить систему
вне зависимости от повреждения суперблока 
raid или других видов отказов и случаев, которые мне не понятны.
</LI>
<LI><CODE>/dev/hda3</CODE> и <CODE>/dev/hdc3</CODE> будут зеркалами
<CODE>/dev/md0</CODE>.</LI>
<LI><CODE>/dev/hda4</CODE> и <CODE>/dev/hdc4</CODE> будут зеркалами
<CODE>/dev/md1</CODE>.
</LI>
<LI> мы выбрали <CODE>/var</CODE> и <CODE>/home</CODE> для зеркализации,
и в разных разделах, основываясь на следующей логике:
<UL>
<LI><CODE>/</CODE> (корневой раздел ) будет содержать
относительно статическую, не изменяющуюся  информацию:
для всех практических применений, он должен быть
только для чтения, без фактической отметки и
монтирования только для чтения.</LI>
<LI><CODE>/home</CODE> должен содержать ''медленно изменяющиеся'' данные.</LI>
<LI><CODE>/var</CODE> должен содержать быстро изменяющиеся данные,
включая спул почты, содержимое баз данных и логи web сервера.</LI>
</UL>

Идея использования нескольких отдельных разделов такова
<B>если</B>, по некоторой странной причине,
при ошибках человека, пропадении питания, или ошибках операционной системы
происходят повреждения - они ограничиваются одним разделом.
Типичный случай - исчезновение питания при записи на диск.
Это должно привести к повреждению файловой системы, что
должно быть исправлено программой <CODE>fsck</CODE> при следующей загрузке.  Если даже 
<CODE>fsck</CODE> делает восстановление 
без создания дополнительных повреждений этим восстановлением, 
можно утешиться тем, что любые повреждения были ограничены
одним разделом.  В другом типичном случае
системный администратор делает ошибку в процессе операции восстановления,
что приводит к стиранию и разрушению всех данных.  Разделы могут 
помочь ограничить влияние ошибок оператора.</LI>
<LI>Разумно обдумать размещение разделов
<CODE>/usr</CODE> или <CODE>/opt</CODE>.  В общем, <CODE>/opt</CODE>
и <CODE>/home</CODE>  - лучший выбор для RAID-5 
разделов, если есть еще диски.  Предостережение:
<B>НЕ</B> помещайте <CODE>/usr</CODE> в RAID-5
раздел.  В случае серьезного отказа, вы можете обнаружить, что не
можете примонтировать <CODE>/usr</CODE>, и необходимый
набор утилит на нем (таких как сетевые утилиты или компилятор.) 
С RAID-1, если произошел отказ, и Вы не можете заставить RAID работать, Вы можете,
по крайней мере, смонтировать одно из двух зеркал. Вы не можете сделать это с любым
другим уровнем RAID (RAID-5, striping, или линейным соединением).
</LI>
</UL>


<P>Итак, чтобы завершить ответ на вопрос:
<UL>
<LI>устанавливаем ОС на диск 1, раздел 1.
Не монтируем любые другие разделы. </LI>
<LI>устанавливаем по инструкции RAID.</LI>
<LI>конфигурирует <CODE>md0</CODE> и <CODE>md1</CODE>.</LI>
<LI>убеждаемся, что знаем что делать в случае отказа!
Делаем ошибку администратора сейчас и 
не ждем реального кризиса.
Эксперимент!
(мы выключаем питание при дисковой активности &mdash;
это нехорошо, но показательно).</LI>
<LI>делаем несколько плохих mount/copy/unmount/rename/reboot для
записи <CODE>/var</CODE> на <CODE>/dev/md1</CODE>.
Делайте старательно, это не опасно.</LI>
<LI>наслаждайтесь!</LI>
</UL>
</BLOCKQUOTE>

    </LI>
<LI><B>В</B>:
Какое различие между <CODE>mdadd</CODE>, <CODE>mdrun</CODE>, 
<I>и т.д.</I> командами, и <CODE>raidadd</CODE>, <CODE>raidrun</CODE> 
командами?
<BLOCKQUOTE>
<B>О</B>:
Имена утилит сменились начиная с релиза 0.5 пакета raidtools. <CODE>md</CODE> 
схема именования использовалась в 0.43 и более старых версиях, 
в то время как <CODE>raid</CODE> используется в 0.5 и более новых версиях.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Я хочу запустить RAID-linear/RAID-0 на 2.0.34 ядре .
Я не хочу применять raid патчи, так как они не нужны для 
RAID-0/linear.  Где я могу взять raid-утилиты для управления?
<BLOCKQUOTE>
<B>О</B>:
Это трудный вопрос, в самом деле, новый пакет raid утилит
при сборке требует установленных патчей RAID-1,4,5. Я не знаю ни одной предкомпилированной
двоичной версии raid утилит, которые доступны на текущий момент.
Однако, эксперименты показывают, что бинарники raid утилит, когда 
скомпилированы с ядром 2.1.100, кажется хорошо работающими 
при создании RAID-0/linear раздела под 2.0.34.  Смельчаки спрашивали об этом, и я <B>временно</B>
поместил бинарники mdadd, mdcreate, и т.д. 
на http://linas.org/linux/Software-RAID/
Вы должны взять man страницы, и т. д. с обычного пакета утилит.
</BLOCKQUOTE>

    </LI>
<LI><B>В</B>:
Могу ли я strip/зеркализировать корневой раздел (<CODE>/</CODE>)?
Почему я не могу загружать Linux прямо с <CODE>md</CODE> диска?

<BLOCKQUOTE>
<B>О</B>:
И LILO и Loadlin требуют не stripped/mirrored раздел
для считывания образа ядра. Если Вы хотите strip/зеркализировать
корневой раздел (<CODE>/</CODE>),
вы должны создать не striped/mirrored раздел для хранения ядра(ядер).
Обычно, этот раздел называют <CODE>/boot</CODE>.
Тогда Вы должны либо использовать начальную поддержку виртуального диска(initrd),
или патчи от Harald Hoyer
&lt;
<A HREF="mailto:HarryH@Royal.Net">HarryH@Royal.Net</A>&gt;
которые позволяют использовать stripped раздел, как корневой раздел.
(Эти патчи - стандартная часть последних ядер серии 2.1.x)

<P>Существуют несколько подходов, которые могут быть использованы.
Один подход детально документирован в Bootable RAID mini-HOWTO:
<A HREF="ftp://ftp.bizsystems.com/pub/raid/bootable-raid">ftp://ftp.bizsystems.com/pub/raid/bootable-raid</A>.
<P>
<P>Как альтернативу, используйте <CODE>mkinitrd</CODE> для построения образа ramdisk,
как показано ниже.
<P>
<P>Edward Welbon
&lt;
<A HREF="mailto:welbon@bga.com">welbon@bga.com</A>&gt;
написал:
<UL>
<LI>... все, что нужно - скрипт для управления установкой.
Для монтирования <CODE>md</CODE> файловой системы как корневой,
главное - построить начальный образ файловой системы,
который содержит необходимые модули и md утилиты для запуска <CODE>md</CODE>. 
У меня есть простой скрипт, который это делает.</LI>
</UL>

<UL>
<LI>Для загрузочной среды, у меня есть маленький <B>дешевый</B> SCSI диск
(170MB я получил его за 20долларов ).
Этот диск работает на AHA1452, им также может быть недорогой 
IDE диск на родном IDE интерфейсе.
От этого диска не требуется скорости, так как он предназначен, в основном, для загрузки.</LI>
</UL>

<UL>
<LI>На диске создана маленькая файловая система содержащая ядро и 
образ <CODE>initrd</CODE>.
Начальной файловой системы должно хватать для загрузки модуля драйвера raid SCSI устройства
и запуска raid раздела, который будет корневым.
Тогда я делаю
<BLOCKQUOTE><CODE>
<PRE>
echo 0x900 > /proc/sys/kernel/real-root-dev
              
</PRE>
</CODE></BLOCKQUOTE>

(<CODE>0x900</CODE> для <CODE>/dev/md0</CODE>)
и выхожу из <CODE>linuxrc</CODE>.
Далее загрузка продолжается обычно. </LI>
</UL>

<UL>
<LI>Я собрал большинство функций как модули кроме драйвера AHA1452,
который будит файловую систему <CODE>initrd</CODE>.
Таким образом у меня очень маленькое ядро. Этот метод простой и надежный,
я делаю так с  2.1.26 и никогда не было проблем, которых не мог бы запросто решить.
Файловая система даже выжила несколько 2.1.4[45] тяжелых 
разрушений без реальных проблем.</LI>
</UL>

<UL>
<LI>В одно время у меня  были размечены raid диски так, что начальные цилиндры
первого raid диска содержали ядро и начальные цилиндры 
второго raid диска содержали образ начальной файловой системы,
вместо этого я использовал начальные цилиндры raid дисков
для подкачкм, так как они более быстрые цилиндры  (зачем терять их на загрузку?).</LI>
</UL>

<UL>
<LI>Хорошо иметь недорогой диск для загрузки, так как 
с него просто загрузиться и, при необходимости, можно использовать как 
восстановительный диск. Если Вы интересуетесь, Вы можете взглянуть 
на скрипт, который создает мой начальный образ ramdisk и потом запускает <CODE>LILO</CODE>.
<BLOCKQUOTE><CODE>
<A HREF="http://www.realtime.net/~welbon/initrd.md.tar.gz">http://www.realtime.net/~welbon/initrd.md.tar.gz</A></CODE></BLOCKQUOTE>

Его достаточно для того, чтобы обрисовать картину.
Он не очень хорош, и, конечно, можно создать более
маленький образ файловой системы для начального ramdisk.
Было бы проще создать его более действенным.
Но он использует <CODE>LILO</CODE> как есть.
Если вы сделаете любые усовершенствования, пожалуйста, отправьте копию мне. 8-) </LI>
</UL>
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Я слышал, что я могу запустить зеркализацию поверх striping. Это правда?
Могу ли я запускать зеркализацию поверх петлевого устройства?
<BLOCKQUOTE>
<B>О</B>:
Да, но не наоборот.  Вы можете поместить stripe поверх
нескольких дисков, и затем строить зеркализацию на базе этого.  Однако,
striping не может быть помещен на зеркало.

<P>Короткое техническое объяснение этого - особенностью linear и stripe 
является использования <CODE>ll_rw_blk</CODE> процедуры для доступа.
<CODE>ll_rw_blk</CODE> процедура
отображает дисковые устройства и сектора, но не блоки.  Блочные устройства могут
быть размещены одно поверх другого; но устройства, которые делают прямой, низкоуровневый
доступ к дискам, такие как <CODE>ll_rw_blk</CODE>, не могут.
<P>
<P>На текущий момент (Ноябрь 1997) RAID не может быть создан на
петлевом (loopback) устройстве, однако возможно, это скоро будет исправлено.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
У меня есть два маленьких диска и три больших диска.  Могу ли я
соединить два маленьких диска в RAID-0, и затем создать
RAID-5 из этого и больших дисков?
<BLOCKQUOTE>
<B>О</B>:
Сейчас (Ноябрь 1997), для массива RAID-5, нет.
Сейчас, это можно сделать только для RAID-1 повер объединенных дисков.
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Какая разница между RAID-1 и RAID-5 для двух дисковой конфигурации
(имеется в виду разница между массивом RAID-1 построенном на двух дисках,
и массивом RAID-5 построенном на двух дисках)?

<BLOCKQUOTE>
<B>О</B>:
Нет разницы в емкости. Также нельзя добавить диски 
ни в один из массивов для увеличения емкости (для деталей, смотрите вопрос ниже).
        
<P>RAID-1 предоставляет преимущество в производительности чтения: драйвер RAID-1
использует технологию распределенного чтения для одновременного чтения двух секторов,
по одному с каждого устройства, это удваивает скорость считывания.
<P>
<P>Драйвер RAID-5, хотя и содержит много оптимизаций, сейчас (Сентябрь 1997) не реализует
то, что паритетный диск - фактически зеркальная копия диска с данными.  Таким образом,
выполняется последовательное чтение данных.
</BLOCKQUOTE>


</LI>
<LI><B>В</B>:
Как я могу защититься от отказа двух дисков?

<BLOCKQUOTE>
<B>О</B>:
Некоторые из алгоритмов RAID дают отказоустойчивость при отказе нескольких дисков,
но на данный момент это не реализовано в Linux.
Однако, программный RAID Linux может защитить от множественных отказов дисков
размещая массив поверх массива.  Например, девять дисков могут быть использованы для создания
трех массивов raid-5.
Затем, эти три массива могут быть объединены в один массив RAID-5.  Фактически, этот тип
конфигурации защищает от отказа трех дисков.  Заметьте, что много дискового пространства
''тратится'' на избыточность информации.

<BLOCKQUOTE><CODE>
<PRE>
    Для NxN массива raid-5,
    N=3, 5 из 9 дисков используется для паритета (=55%)
    N=4, 7 из 16 дисков
    N=5, 9 из 25 дисков
    ...
    N=9, 17 из 81 дисков (=~20&amp;процентов;)
            
</PRE>
</CODE></BLOCKQUOTE>

В общем, массив MxN будет использовать M+N-1 дисков на паритет.
Наименьшее количество пространства "теряется", когда M=N.
      
<P>Другая альтернатива создать массив RAID-1 с
тремя дисками.  Заметьте, что все три диска содержат идентичные данные, и 2/3 пространства ''теряется''.
<P>
</BLOCKQUOTE>

        </LI>
<LI><B>В</B>:
Я  хочу понять, существует-ли что-то типа <CODE>fsck</CODE>: 
если раздел не был правильно демонтирован, 
<CODE>fsck</CODE> запускается и исправляет файловую систему более  90% 
времени. Так как машина способна исправлять это сама 
с помощью <CODE>ckraid --fix</CODE>, почему не автоматизировать это?


<BLOCKQUOTE>
<B>О</B>:
Это возможно сделать добавлением следующие строки в 
<CODE>/etc/rc.d/rc.sysinit</CODE>:
<PRE>
    mdadd /dev/md0 /dev/hda1 /dev/hdc1 || {
        ckraid --fix /etc/raid.usr.conf
        mdadd /dev/md0 /dev/hda1 /dev/hdc1
    }
            
</PRE>

или
<PRE>
    mdrun -p1 /dev/md0
    if [ $? -gt 0 ] ; then
            ckraid --fix /etc/raid1.conf
            mdrun -p1 /dev/md0
    fi
            
</PRE>

Перед предоставлением более полного и надежного скрипта,
рассмотрим теорию операций.

Gadi Oxman написал:
При неправильном завершении, Linux может быть в одном их следующих состояний:
<UL>
<LI>При возникновении аварийного завершения дисковый кеш в 
памяти был синхронизирован с RAID набором; потерь данных нет.
</LI>
<LI>При возникновении аварийного завершения в памяти дискового 
кеша было более новое содержимое, чем в RAID наборе; в 
результате повреждена файловая система и возможно потеряны 
данные.      
Это состояние может быть далее разделено на два других 
состояния:
      
<UL>
<LI>При аварийном завершении Linux записывал данные.</LI>
<LI>При аварийном завершении Linux не записывал данные.</LI>
</UL>
</LI>
</UL>


Допустим мы используем массив RAID-1. В (2a), может случиться, что перед аварией
небольшое количество блоков данных было успешно записано 
только на несколько из зеркал, таким образом при следующей загрузке,
зеркала уже не будут идентичными.
      
Если мы проигнорировали разницу в зеркалах, the raidtools-0.36.3 
код балансировки чтения может выбрать для чтения
блоки из любого зеркала,
что приведет к противоречивому поведению (например, вывод 
<CODE>e2fsck -n /dev/md0</CODE> будет отличаться от запуска к запуску).
      
<P>Так как RAID не защищает от неправильного завершения, обычно
нет никакого ''целиком корректного'' пути для устранения разницы в зеркалах
и повреждений файловой системы.
<P>Например, по умолчанию <CODE>ckraid --fix</CODE> будет выбирать
содержимое первого действующего зеркала и обновлять другие зеркала.
Однако, в зависимости от точного времени аварии, 
данные на другом зеркале могут быть более свежие, 
и мы можем пожелать использовать их как источник для 
восстановления зеркал, или, возможно, использовать другой метод восстановления.
<P>Следующий скрипт реализует одну из самых здравых последовательностей 
загрузки.  В частности, он принимает меры предосторожности 
длинными, повторяющимися <CODE>ckraid</CODE>-ов при не совместных дисках, 
контроллерах, или драйверах 
контроллеров дисков.  Модифицируйте его, для соответствия своей 
конфигурации,
и скопируйте в <CODE>rc.raid.init</CODE>.  Затем вызовите
<CODE>rc.raid.init</CODE> после проверки fsck-ом монтирования rw 
корневого раздела, но перед 
проверкой fsck-ом оставшихся разделов.  Убедитесь, что текущий 
каталог в путях поиска (переменная PATH).
<PRE>
    mdadd /dev/md0 /dev/hda1 /dev/hdc1 || {
        rm -f /fastboot             # force an fsck to occur  
        ckraid --fix /etc/raid.usr.conf
        mdadd /dev/md0 /dev/hda1 /dev/hdc1
    }
    # if a crash occurs later in the boot process,
    # we at least want to leave this md in a clean state.
    /sbin/mdstop /dev/md0

    mdadd /dev/md1 /dev/hda2 /dev/hdc2 || {
        rm -f /fastboot             # force an fsck to occur  
        ckraid --fix /etc/raid.home.conf
        mdadd /dev/md1 /dev/hda2 /dev/hdc2
    }
    # if a crash occurs later in the boot process,
    # we at least want to leave this md in a clean state.
    /sbin/mdstop /dev/md1

    mdadd /dev/md0 /dev/hda1 /dev/hdc1
    mdrun -p1 /dev/md0
    if [ $? -gt 0 ] ; then
        rm -f /fastboot             # force an fsck to occur  
        ckraid --fix /etc/raid.usr.conf
        mdrun -p1 /dev/md0
    fi
    # if a crash occurs later in the boot process,
    # we at least want to leave this md in a clean state.
    /sbin/mdstop /dev/md0

    mdadd /dev/md1 /dev/hda2 /dev/hdc2
    mdrun -p1 /dev/md1
    if [ $? -gt 0 ] ; then
        rm -f /fastboot             # force an fsck to occur  
        ckraid --fix /etc/raid.home.conf
        mdrun -p1 /dev/md1
    fi
    # if a crash occurs later in the boot process,
    # we at least want to leave this md in a clean state.
    /sbin/mdstop /dev/md1

    # OK, just blast through the md commands now.  If there were
    # errors, the above checks should have fixed things up.
    /sbin/mdadd /dev/md0 /dev/hda1 /dev/hdc1
    /sbin/mdrun -p1 /dev/md0
    
    /sbin/mdadd /dev/md12 /dev/hda2 /dev/hdc2
    /sbin/mdrun -p1 /dev/md1

            
</PRE>

В дополнение к указанному, Вы должны создать 
<CODE>rc.raid.halt</CODE>, который должен выглядеть как показано ниже:
<PRE>
    /sbin/mdstop /dev/md0
    /sbin/mdstop /dev/md1
            
</PRE>

Модифицируйте оба <CODE>rc.sysinit</CODE> и
<CODE>init.d/halt</CODE> для включения этого в место, где файловая система уже демонтирована
при halt/reboot.  (Заметьте
что <CODE>rc.sysinit</CODE> демонтирует и перезагружает если  <CODE>fsck</CODE>
завершился с ошибкой.)
<P>
</BLOCKQUOTE>

</LI>
<LI><B>В</B>:
Могу я установить одну половину RAID-1 зеркала на один диск, который есть у меня сейчас и
затем позже взять другой диск и просто его добавить?

<BLOCKQUOTE>
<B>О</B>:
С текущими утилитами - нет, во всяком случае не простым способом.  В частности,
вы не можете просто скопировать содержимое одного диска на другой и затем их спаровать.
Это потому, что драйвера RAID используют часть пространства в конце раздела для размещения 
суперблока.  Это слегка уменьшает количество пространства, доступного для файловой системы;
если Вы просто попробуете принудительно
поставить  RAID-1 на раздел с существующей файловой системой, 
raid суперблок перезапишет часть файловой системы и обрубит данные.
Так как ext2fs файловая система разбрасывает фалы по разделу случайным образом
(для избежания фрагментации), есть хороший шанс, что файл будет 
лежать в самом конце раздела  перед окончанием диска.

<P>Если Вы сообразительны, я предлагаю Вам вычислить сколько места нужно под суперблок
RAID, и сделать вашу файловую систему немного короче,
оставив место на перспективу.
Но тогда, если вы такой умный, Вы должны также быть способны модифицировать
утилиты для автоматизации этого процесса.
(Утилиты не так уж сложны).
<P>
<P><B>Заметка:</B> Внимательный читатель заметит, что следующий трюк может 
сработать; я не пытался проверить это:
Сделайте <CODE>mkraid</CODE>с <CODE>/dev/null</CODE>, как одним из устройств.
Тогда <CODE>mdadd -r</CODE>с только одним, истинным диском 
(не делайте mdadd <CODE>/dev/null</CODE>).  <CODE>mkraid</CODE>
должен быть успешно создать raid массив, когда 
mdadd шаг выполняется - файловая система запущена в "деградированном" режиме, 
как если бы один из дисков отказал.
</BLOCKQUOTE>

</LI>
</OL>
<HR>
<A HREF="Software-RAID-HOWTO-2.2-4.html">Next</A>
<A HREF="Software-RAID-HOWTO-2.2-2.html">Previous</A>
<A HREF="Software-RAID-HOWTO-2.2.html#toc3">Contents</A>
</BODY>
</HTML>
